{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d0b2b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ face_landmarker.task ready.\n",
      "âš¡ hand_landmarker.task ready.\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: Model Download\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Define URLs\n",
    "face_url = \"https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task\"\n",
    "hand_url = \"https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task\"\n",
    "\n",
    "def download(url, name):\n",
    "    if not os.path.exists(name):\n",
    "        print(f\"ðŸ“¥ Downloading {name}...\")\n",
    "        urllib.request.urlretrieve(url, name)\n",
    "        print(\"âœ… Done.\")\n",
    "    else:\n",
    "        print(f\"âš¡ {name} ready.\")\n",
    "\n",
    "download(face_url, \"face_landmarker.task\")\n",
    "download(hand_url, \"hand_landmarker.task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbf80f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MediaPipe Version: 0.10.31\n",
      "âœ… Imports loaded. You can now run the video cells.\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: Critical Imports\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(f\"MediaPipe Version: {mp.__version__}\")\n",
    "print(\"âœ… Imports loaded. You can now run the video cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "775bf083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. SETUP\n",
    "base_options = python.BaseOptions(model_asset_path='face_landmarker.task')\n",
    "options = vision.FaceLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    output_face_blendshapes=True,\n",
    "    output_facial_transformation_matrixes=True,\n",
    "    running_mode=vision.RunningMode.VIDEO, # <--- CRITICAL FIX\n",
    "    num_faces=1)\n",
    "\n",
    "detector = vision.FaceLandmarker.create_from_options(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b457fba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to stop.\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: Face Mesh\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "print(\"Press 'q' to stop.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        \n",
    "        # 2. CONVERT\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        timestamp = int(time.time() * 1000)\n",
    "        \n",
    "        # 3. DETECT\n",
    "        result = detector.detect_for_video(mp_image, timestamp)\n",
    "        \n",
    "        # 4. DRAW\n",
    "        if result.face_landmarks:\n",
    "            for landmarks in result.face_landmarks:\n",
    "                for lm in landmarks:\n",
    "                    x, y = int(lm.x * frame.shape[1]), int(lm.y * frame.shape[0])\n",
    "                    cv2.circle(frame, (x, y), 1, (0, 255, 255), -1)\n",
    "\n",
    "        cv2.imshow('Face Mesh', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    detector.close() # cleanup AI memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af0c9125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to stop.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CELL 4: Hand Tracking\n",
    "# 1. SETUP\n",
    "base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "options = vision.HandLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    num_hands=2,\n",
    "    running_mode=vision.RunningMode.VIDEO, # <--- CRITICAL FIX\n",
    "    min_hand_detection_confidence=0.5)\n",
    "\n",
    "detector = vision.HandLandmarker.create_from_options(options)\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "print(\"Press 'q' to stop.\")\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "\n",
    "        # 2. CONVERT\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        timestamp = int(time.time() * 1000)\n",
    "        \n",
    "        # 3. DETECT\n",
    "        result = detector.detect_for_video(mp_image, timestamp)\n",
    "        \n",
    "        # 4. DRAW\n",
    "        if result.hand_landmarks:\n",
    "            for landmarks in result.hand_landmarks:\n",
    "                for lm in landmarks:\n",
    "                    x, y = int(lm.x * frame.shape[1]), int(lm.y * frame.shape[0])\n",
    "                    cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)\n",
    "\n",
    "        cv2.imshow('Hand Tracking', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    detector.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f712d2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Cell 4 first to capture some hand data!\n"
     ]
    }
   ],
   "source": [
    "# CELL 5: 3D Depth Analysis Chart\n",
    "if 'detection_result' in locals() and detection_result.hand_landmarks:\n",
    "    landmarks = detection_result.hand_landmarks[0]\n",
    "    \n",
    "    # Extract Z-coordinates (Depth relative to wrist)\n",
    "    z_vals = [lm.z for lm in landmarks]\n",
    "    indices = range(len(z_vals))\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(indices, z_vals, color='purple')\n",
    "    plt.title(\"Hand Depth Profile (How far is each finger?)\")\n",
    "    plt.xlabel(\"Landmark Index (0=Wrist, 8=Index Tip)\")\n",
    "    plt.ylabel(\"Relative Depth (Z-Axis)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Run Cell 4 first to capture some hand data!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
